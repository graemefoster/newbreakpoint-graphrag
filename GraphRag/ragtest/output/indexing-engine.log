17:41:13,480 graphrag.index.cli INFO Logging enabled at /Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output/indexing-engine.log
17:41:13,481 graphrag.index.cli INFO Starting pipeline run for: 20241009-174113, dryrun=False
17:41:13,482 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "azure_openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://aoai.openai.azure.com/",
        "api_version": "2024-04-01-preview",
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": "gpt-4o",
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 30,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 1
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "/Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_embedding",
            "model": "text-embedding-ada-002",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "text-embedding-ada-002",
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 60,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:41:13,485 graphrag.index.create_pipeline_config INFO skipping workflows 
17:41:13,485 graphrag.index.run.run INFO Running pipeline
17:41:13,485 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output
17:41:13,486 graphrag.index.input.load_input INFO loading input from root_dir=input
17:41:13,486 graphrag.index.input.load_input INFO using file storage for input
17:41:13,487 graphrag.index.storage.file_pipeline_storage INFO search /Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/input for files matching .*\.txt$
17:41:13,487 graphrag.index.input.text INFO found text files from input, found [('sample-text.txt', {})]
17:41:13,510 graphrag.index.input.text INFO Found 1 files, loading 1
17:41:13,513 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
17:41:13,513 graphrag.index.run.run INFO Final # of rows loaded: 1
17:41:13,582 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:41:13,583 datashaper.workflow.workflow INFO executing verb orderby
17:41:13,587 datashaper.workflow.workflow INFO executing verb zip
17:41:13,589 datashaper.workflow.workflow INFO executing verb aggregate_override
17:41:13,592 datashaper.workflow.workflow INFO executing verb chunk
17:41:13,701 datashaper.workflow.workflow INFO executing verb select
17:41:13,703 datashaper.workflow.workflow INFO executing verb unroll
17:41:13,706 datashaper.workflow.workflow INFO executing verb rename
17:41:13,708 datashaper.workflow.workflow INFO executing verb genid
17:41:13,710 datashaper.workflow.workflow INFO executing verb unzip
17:41:13,711 datashaper.workflow.workflow INFO executing verb copy
17:41:13,713 datashaper.workflow.workflow INFO executing verb filter
17:41:13,726 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
17:41:13,826 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
17:41:13,827 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
17:41:13,845 datashaper.workflow.workflow INFO executing verb entity_extract
17:41:13,860 graphrag.llm.openai.create_openai_client INFO Creating Azure OpenAI client api_base=https://aoai.openai.azure.com, deployment_name=gpt-4o
17:41:13,878 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=0, RPM=30
17:41:13,878 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 1
17:41:33,120 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:41:33,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.243703083950095. input_tokens=3525, output_tokens=1911
17:41:54,183 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:41:54,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.05900708399713. input_tokens=3524, output_tokens=1845
17:41:54,281 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:41:54,283 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 21 seconds. Follow recommendation? True
17:42:40,398 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:42:40,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 23.85963508405257. input_tokens=3525, output_tokens=2340
17:43:08,462 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:43:08,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.05656395794358. input_tokens=3525, output_tokens=2675
17:43:08,549 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:43:08,550 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 9 seconds. Follow recommendation? True
17:53:51,294 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [person, artwork, location, historical_event, art_movement, institution, relationship, technique]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\nFormat each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in The primary language of the provided text is "English." as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. If you have to translate into The primary language of the provided text is "English.", just translate the descriptions, nothing else!\n\n5. When finished, output <|COMPLETE|>.\n\n-Examples-\n######################\n\nExample 1:\n\nentity_types: [person, artwork, location, historical_event, art_movement, institution, relationship, technique]\ntext:\n age. The young\nDuchess, who at one time owned as many as eighty-four splendid gowns,\nrefused to wear a certain dress of woven gold, which her husband had\ngiven her, if Cecilia Gallerani, the Sappho of her day, continued to\nwear a very similar one, which presumably had been given to her by\nLudovico. Having discarded Cecilia, who, as her tastes did not lie in\nthe direction of the Convent, was married in 1491 to Count Ludovico\nBergamini, the Duke in 1496 became enamoured of Lucrezia Crivelli, a\nlady-in-waiting to the Duchess Beatrice.\n\nLeonardo, as court painter, perhaps painted a portrait, now lost, of\nLucrezia, whose features are more likely to be preserved to us in the\nportrait by Ambrogio da Predis, now in the Collection of the Earl\n------------------------\noutput:\n("entity"<|>DUCHESS<|>person<|>The young Duchess who owned as many as eighty-four splendid gowns and had a conflict with Cecilia Gallerani over a dress of woven gold)\n##\n("entity"<|>CECILIA GALLERANI<|>person<|>Cecilia Gallerani, known as the Sappho of her day, who was involved in a conflict with the Duchess over a dress and was married to Count Ludovico Bergamini in 1491)\n##\n("entity"<|>LUDOVICO<|>person<|>Ludovico, who presumably gave a dress to Cecilia Gallerani and later became enamored of Lucrezia Crivelli)\n##\n("entity"<|>COUNT LUDOVICO BERGAMINI<|>person<|>Count Ludovico Bergamini, who married Cecilia Gallerani in 1491)\n##\n("entity"<|>LUCREZIA CRIVELLI<|>person<|>Lucrezia Crivelli, a lady-in-waiting to the Duchess Beatrice, who became the object of Ludovico\'s affection in 1496)\n##\n("entity"<|>LEONARDO<|>person<|>Leonardo da Vinci, the court painter who perhaps painted a now-lost portrait of Lucrezia Crivelli)\n##\n("entity"<|>AMBROGIO DA PREDIS<|>person<|>Ambrogio da Predis, the artist who likely preserved Lucrezia Crivelli\'s features in a portrait now in the Collection of the Earl)\n##\n("entity"<|>COLLECTION OF THE EARL<|>location<|>The collection where Ambrogio da Predis\'s portrait of Lucrezia Crivelli is currently held)\n##\n("relationship"<|>DUCHESS<|>CECILIA GALLERANI<|>The Duchess refused to wear a dress of woven gold if Cecilia Gallerani continued to wear a similar one<|>7)\n##\n("relationship"<|>CECILIA GALLERANI<|>LUDOVICO<|>Cecilia Gallerani presumably received a dress from Ludovico<|>6)\n##\n("relationship"<|>CECILIA GALLERANI<|>COUNT LUDOVICO BERGAMINI<|>Cecilia Gallerani married Count Ludovico Bergamini in 1491<|>8)\n##\n("relationship"<|>LUDOVICO<|>LUCREZIA CRIVELLI<|>Ludovico became enamored of Lucrezia Crivelli in 1496<|>7)\n##\n("relationship"<|>LUCREZIA CRIVELLI<|>DUCHESS<|>Lucrezia Crivelli was a lady-in-waiting to the Duchess Beatrice<|>5)\n##\n("relationship"<|>LEONARDO<|>LUCREZIA CRIVELLI<|>Leonardo da Vinci perhaps painted a now-lost portrait of Lucrezia Crivelli<|>6)\n##\n("relationship"<|>AMBROGIO DA PREDIS<|>LUCREZIA CRIVELLI<|>Ambrogio da Predis likely preserved Lucrezia Crivelli\'s features in a portrait<|>6)\n##\n("relationship"<|>AMBROGIO DA PREDIS<|>COLLECTION OF THE EARL<|>Ambrogio da Predis\'s portrait of Lucrezia Crivelli is now in the Collection of the Earl<|>5)\n<|COMPLETE|>\n#############################\n\n\nExample 2:\n\nentity_types: [person, artwork, location, historical_event, art_movement, institution, relationship, technique]\ntext:\n.\n\nLeonardo has succeeded in producing the effect of the _coup de\nthéâtre_ at the moment when Jesus said "One of you shall betray\nme." Instantly the various apostles realise that there is a traitor\namong their number, and show by their different gestures their\ndifferent passions, and reveal their different temperaments. On the\nleft of Christ is St. John who is overcome with grief and is\ninterrogated by the impetuous Peter, near whom is seated Judas\nIscariot who, while affecting the calm of innocence, is quite unable\nto conceal his inner feelings; he instinctively clasps the money-bag\nand in so doing upsets the salt-cellar.\n\nIt will be remembered that the Prior of the Convent complained to\nLudovico Sforza, Duke of Milan, that Leonardo was taking too long to\npaint the fresco and was causing the Convent considerable\ninconvenience\n------------------------\noutput:\n("entity"<|>LEONARDO<|>PERSON<|>Leonardo da Vinci, the renowned Italian artist, who painted the famous fresco "The Last Supper")\n##\n("entity"<|>THE LAST SUPPER<|>ARTWORK<|>A famous fresco by Leonardo da Vinci depicting the moment Jesus announces that one of his apostles will betray him)\n##\n("entity"<|>JESUS<|>PERSON<|>Central figure in "The Last Supper" who announces the betrayal)\n##\n("entity"<|>ST. JOHN<|>PERSON<|>Apostle depicted in "The Last Supper" who is overcome with grief)\n##\n("entity"<|>PETER<|>PERSON<|>Apostle depicted in "The Last Supper" who is impetuous and interrogates St. John)\n##\n("entity"<|>JUDAS ISCARIOT<|>PERSON<|>Apostle depicted in "The Last Supper" who betrays Jesus and is shown clasping a money-bag)\n##\n("entity"<|>LUDOVICO SFORZA<|>PERSON<|>Duke of Milan who was approached by the Prior of the Convent regarding the delay in the painting of "The Last Supper")\n##\n("entity"<|>DUKE OF MILAN<|>TITLE<|>Title held by Ludovico Sforza, who was involved in the situation regarding the painting of "The Last Supper")\n##\n("entity"<|>CONVENT<|>INSTITUTION<|>The religious institution where "The Last Supper" was being painted)\n##\n("relationship"<|>LEONARDO<|>THE LAST SUPPER<|>Leonardo da Vinci is the artist who painted "The Last Supper"<|>10)\n##\n("relationship"<|>JESUS<|>THE LAST SUPPER<|>Jesus is the central figure in "The Last Supper" painting<|>9)\n##\n("relationship"<|>ST. JOHN<|>THE LAST SUPPER<|>St. John is one of the apostles depicted in "The Last Supper"<|>8)\n##\n("relationship"<|>PETER<|>THE LAST SUPPER<|>Peter is one of the apostles depicted in "The Last Supper"<|>8)\n##\n("relationship"<|>JUDAS ISCARIOT<|>THE LAST SUPPER<|>Judas Iscariot is the apostle who betrays Jesus, depicted in "The Last Supper"<|>9)\n##\n("relationship"<|>LUDOVICO SFORZA<|>CONVENT<|>Ludovico Sforza was approached by the Prior of the Convent regarding the delay in the painting of "The Last Supper"<|>7)\n##\n("relationship"<|>DUKE OF MILAN<|>LUDOVICO SFORZA<|>Ludovico Sforza held the title of Duke of Milan<|>10)\n<|COMPLETE|>\n#############################\n\n\n\n-Real Data-\n######################\nentity_types: [person, artwork, location, historical_event, art_movement, institution, relationship, technique]\ntext: head of\nthe angel is, however, magnificently painted, and by Leonardo; the\npanel, taken as a whole, is exceedingly beautiful and full of charm\nand tenderness.\n\n\n\n\nTHE LAST SUPPER\n\nBetween 1496 and 1498 Leonardo painted his _chef d\'oeuvre_, the\n"Last Supper," (Plate IV.) for the end wall of the Refectory of the\nDominican Convent of S. Maria delle Grazie at Milan. It was originally\nexecuted in tempera on a badly prepared stucco ground and began to\ndeteriorate a very few years after its completion. As early as 1556 it\nwas half ruined. In 1652 the monks cut away a part of the fresco\nincluding the feet of the Christ to make a doorway. In 1726 one\nMichelangelo Belotti, an obscure Milanese painter, received £300 for\nthe worthless labour he bestowed on restoring it. He seems to have\nemployed some astringent restorative which revived the colours\ntemporarily, and then left them in deeper eclipse than before. In 1770\nthe fresco was again restored by Mazza. In 1796 Napoleon\'s cavalry,\ncontrary to his express orders, turned the refectory into a stable,\nand pelted the heads of the figures with dirt. Subsequently the\nrefectory was used to store hay, and at one time or another it has\nbeen flooded. In 1820 the fresco was again restored, and in 1854 this\nrestoration was effaced. In October 1908 Professor Cavenaghi completed\nthe delicate task of again restoring it, and has, in the opinion of\nexperts, now preserved it from further injury. In addition, the\ndevices of Ludovico and his Duchess and a considerable amount of\nfloral decoration by Leonardo himself have been brought to light.\n\nLeonardo has succeeded in producing the effect of the _coup de\nthéâtre_ at the moment when Jesus said "One of you shall betray\nme." Instantly the various apostles realise that there is a traitor\namong their number, and show by their different gestures their\ndifferent passions, and reveal their different temperaments. On the\nleft of Christ is St. John who is overcome with grief and is\ninterrogated by the impetuous Peter, near whom is seated Judas\nIscariot who, while affecting the calm of innocence, is quite unable\nto conceal his inner feelings; he instinctively clasps the money-bag\nand in so doing upsets the salt-cellar.\n\nIt will be remembered that the Prior of the Convent complained to\nLudovico Sforza, Duke of Milan, that Leonardo was taking too long to\npaint the fresco and was causing the Convent considerable\ninconvenience. Leonardo had his revenge by threatening to paint the\nfeatures of the impatient Prior into the face of  Judas Iscariot. The\nincident has been quaintly told in the following lines:--\n\n  "Padre Bandelli, then, complains of me\n  Because, forsooth, I have not drawn a line\n  Upon the Saviour\'s head; perhaps, then, he\n  Could without trouble paint that head divine.\n  But think, oh Signor Duca, what should be\n  The pure perfection of Our Saviour\'s face--\n  What sorrowing majesty, what noble grace,\n  At that dread moment when He brake the bread,\n  And those submissive words of pathos said:\n\n  "\'By one among you I shall be betrayed,\'--\n  And say if \'tis an easy task to find\n  Even among the best that walk this Earth,\n  The fitting type of that divinest worth,\n  That has its image solely in the mind.\n  Vainly my pencil struggles to express\n  The sorrowing grandeur of such holiness.\n  In patient thought, in ever-seeking prayer,\n  I strive to shape that glorious face within,\n  But the soul\'s mirror, dulled and dimmed by sin,\n  Reflects not yet the perfect image there.\n  Can the hand do before the soul has wrought;\n  Is not our art the servant of our thought?\n\n  "And Judas too, the basest face I see,\n  Will not contain his utter infamy;\n  Among the dregs and offal of mankind\n  Vainly I seek an utter wretch to find.\n  He who for thirty silver coins could sell\n  His Lord, must be the Devil\'s miracle.\n  Padre Bandelli thinks it easy is\n  To find the type of him who with a kiss\n  Betrayed his Lord. Well, what I can I\'ll do;\n  And if it please his reverence and you,\n  For Judas\' face I\'m willing to paint his."\n\n       *       *       *       *       *\n\n  "... I dare not paint\n  Till all is ordered and matured within,\n  Hand-work and head-work have an earthly taint,\n  But when the soul commands I shall begin;\n  On themes like these I should not dare to dwell\n  With our good Prior--they to him would be\n  Mere nonsense; he must touch and taste and see,\n  And facts, he says, are never mystical."\n\n[Illustration: PLATE VI.--THE HEAD OF CHRIST\n\nIn the Brera Gallery, Milan. No. 280. 1 ft. 0-1/2 ins. by\n1 ft. 4 ins. (0.32 x 0.40)]\n\nThe copy of the "Last Supper" (Plate V.) by Marco d\'Oggiono, now in\n######################\noutput:'}
17:54:07,573 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:54:07,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 14.08279804198537. input_tokens=3525, output_tokens=1279
17:54:37,64 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:54:37,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.484478708007373. input_tokens=3525, output_tokens=3040
17:54:37,139 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:54:37,141 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 17 seconds. Follow recommendation? True
17:55:11,647 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:55:11,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 16.110593958059326. input_tokens=3523, output_tokens=1569
17:56:04,931 graphrag.index.cli INFO Logging enabled at /Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output/indexing-engine.log
17:56:04,936 graphrag.index.cli INFO Starting pipeline run for: 20241009-175604, dryrun=False
17:56:04,937 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "azure_openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://aoai.openai.azure.com/",
        "api_version": "2024-04-01-preview",
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": "gpt-4o",
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 30,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 1
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "/Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_embedding",
            "model": "text-embedding-ada-002",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "text-embedding-ada-002",
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 60,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "azure_openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://aoai.openai.azure.com/",
            "api_version": "2024-04-01-preview",
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": "gpt-4o",
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 30,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 1
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:56:04,960 graphrag.index.create_pipeline_config INFO skipping workflows 
17:56:04,960 graphrag.index.run.run INFO Running pipeline
17:56:04,960 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/output
17:56:04,960 graphrag.index.input.load_input INFO loading input from root_dir=input
17:56:04,960 graphrag.index.input.load_input INFO using file storage for input
17:56:04,961 graphrag.index.storage.file_pipeline_storage INFO search /Users/graemefoster/code/github/graemefoster/PoorMansGraphRag/GraphRag/ragtest/input for files matching .*\.txt$
17:56:04,961 graphrag.index.input.text INFO found text files from input, found [('sample-text.txt', {})]
17:56:04,964 graphrag.index.input.text INFO Found 1 files, loading 1
17:56:04,966 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
17:56:04,966 graphrag.index.run.run INFO Final # of rows loaded: 1
17:56:05,47 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:56:05,49 datashaper.workflow.workflow INFO executing verb orderby
17:56:05,52 datashaper.workflow.workflow INFO executing verb zip
17:56:05,54 datashaper.workflow.workflow INFO executing verb aggregate_override
17:56:05,56 datashaper.workflow.workflow INFO executing verb chunk
17:56:05,155 datashaper.workflow.workflow INFO executing verb select
17:56:05,156 datashaper.workflow.workflow INFO executing verb unroll
17:56:05,159 datashaper.workflow.workflow INFO executing verb rename
17:56:05,160 datashaper.workflow.workflow INFO executing verb genid
17:56:05,162 datashaper.workflow.workflow INFO executing verb unzip
17:56:05,163 datashaper.workflow.workflow INFO executing verb copy
17:56:05,165 datashaper.workflow.workflow INFO executing verb filter
17:56:05,193 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
17:56:05,301 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
17:56:05,301 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
17:56:05,323 datashaper.workflow.workflow INFO executing verb entity_extract
17:56:05,343 graphrag.llm.openai.create_openai_client INFO Creating Azure OpenAI client api_base=https://aoai.openai.azure.com, deployment_name=gpt-4o
17:56:05,358 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=0, RPM=30
17:56:05,358 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 1
17:56:31,541 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:56:31,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.182850957964547. input_tokens=3524, output_tokens=2347
17:56:48,651 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:56:48,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.109628167003393. input_tokens=3376, output_tokens=1598
17:56:48,738 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:56:48,741 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 18 seconds. Follow recommendation? True
17:57:17,734 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:57:17,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 9.26941316598095. input_tokens=34, output_tokens=833
17:57:17,811 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:57:17,812 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 14 seconds. Follow recommendation? True
17:57:41,376 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:57:41,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 7.798472041962668. input_tokens=34, output_tokens=733
17:57:41,456 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:57:41,457 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 28 seconds. Follow recommendation? True
17:58:23,753 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:58:23,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 12.654745333013125. input_tokens=34, output_tokens=972
17:58:23,836 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:58:23,837 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 11 seconds. Follow recommendation? True
17:59:01,772 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:59:01,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 24.948957917047665. input_tokens=34, output_tokens=2186
17:59:01,856 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:59:01,858 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 10 seconds. Follow recommendation? True
17:59:28,833 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:59:28,839 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 15.565603041090071. input_tokens=34, output_tokens=1300
17:59:28,914 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:59:28,915 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 9 seconds. Follow recommendation? True
17:59:49,721 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
17:59:49,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 10.328948291949928. input_tokens=34, output_tokens=896
17:59:49,802 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
17:59:49,802 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 24 seconds. Follow recommendation? True
18:00:40,632 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:00:40,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 25.234429667005315. input_tokens=34, output_tokens=2326
18:00:50,445 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:00:50,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.807119791978039. input_tokens=34, output_tokens=976
18:00:50,525 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:00:50,526 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 26 seconds. Follow recommendation? True
18:01:25,470 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:25,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 7.079488915973343. input_tokens=34, output_tokens=656
18:01:25,493 datashaper.workflow.workflow INFO executing verb merge_graphs
18:01:25,545 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
18:01:25,661 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
18:01:25,661 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
18:01:25,671 datashaper.workflow.workflow INFO executing verb summarize_descriptions
18:01:26,543 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:26,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8415800420334563. input_tokens=280, output_tokens=68
18:01:27,515 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:27,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9700251249596477. input_tokens=278, output_tokens=73
18:01:29,49 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:29,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5334285839926451. input_tokens=302, output_tokens=114
18:01:30,73 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:30,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0233891669195145. input_tokens=318, output_tokens=64
18:01:30,142 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:01:30,142 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 11 seconds. Follow recommendation? True
18:01:44,180 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:44,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.1571354999905452. input_tokens=287, output_tokens=69
18:01:45,432 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:45,434 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2506561659974977. input_tokens=320, output_tokens=111
18:01:46,465 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:46,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0326978750526905. input_tokens=282, output_tokens=81
18:01:47,481 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:47,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0146799160866067. input_tokens=335, output_tokens=75
18:01:48,208 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:48,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7278738329187036. input_tokens=319, output_tokens=56
18:01:48,922 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:48,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7104276660829782. input_tokens=330, output_tokens=59
18:01:49,877 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:49,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9548590839840472. input_tokens=313, output_tokens=60
18:01:50,492 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:50,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6139377080835402. input_tokens=281, output_tokens=39
18:01:51,679 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:01:51,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1868403750704601. input_tokens=317, output_tokens=101
18:01:51,746 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:01:51,748 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 28 seconds. Follow recommendation? True
18:02:21,782 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:21,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.011897041928023. input_tokens=293, output_tokens=55
18:02:22,819 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:22,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0337114169960842. input_tokens=337, output_tokens=79
18:02:24,212 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:24,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3915273749735206. input_tokens=425, output_tokens=120
18:02:25,165 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:25,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.951463625067845. input_tokens=324, output_tokens=70
18:02:27,429 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:27,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.262200584053062. input_tokens=303, output_tokens=102
18:02:28,636 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:28,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2046395420329645. input_tokens=423, output_tokens=109
18:02:30,82 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:30,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4448595829308033. input_tokens=309, output_tokens=122
18:02:31,617 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:31,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5332426669774577. input_tokens=303, output_tokens=135
18:02:32,529 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:32,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.908173541072756. input_tokens=311, output_tokens=71
18:02:33,767 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:33,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.238958791946061. input_tokens=299, output_tokens=105
18:02:34,912 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:34,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.144117708900012. input_tokens=314, output_tokens=95
18:02:36,216 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:36,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3019495829939842. input_tokens=298, output_tokens=92
18:02:36,285 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:02:36,287 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 8 seconds. Follow recommendation? True
18:02:46,568 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:46,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.0336826670682058. input_tokens=312, output_tokens=44
18:02:47,693 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:47,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1240214590216056. input_tokens=298, output_tokens=90
18:02:48,775 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:48,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0789550830377266. input_tokens=289, output_tokens=90
18:02:50,151 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:50,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.378926333039999. input_tokens=296, output_tokens=109
18:02:51,273 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:51,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1169916249345988. input_tokens=329, output_tokens=90
18:02:52,372 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:52,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0978300420101732. input_tokens=288, output_tokens=90
18:02:53,330 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:53,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9554807500680909. input_tokens=362, output_tokens=74
18:02:54,43 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:54,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7123650830471888. input_tokens=285, output_tokens=41
18:02:55,447 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:55,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4039578749798238. input_tokens=312, output_tokens=111
18:02:56,602 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:02:56,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1528459589462727. input_tokens=301, output_tokens=73
18:02:56,670 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:02:56,672 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 25 seconds. Follow recommendation? True
18:03:25,381 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:25,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.8126211250200868. input_tokens=331, output_tokens=120
18:03:25,889 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:25,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5055647909175605. input_tokens=277, output_tokens=18
18:03:27,118 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:27,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2270627089310437. input_tokens=320, output_tokens=105
18:03:28,244 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:28,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1259527910733595. input_tokens=343, output_tokens=69
18:03:30,88 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:30,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8432264169678092. input_tokens=314, output_tokens=159
18:03:31,338 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:31,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2486030000727624. input_tokens=307, output_tokens=94
18:03:32,647 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:32,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3076520420145243. input_tokens=301, output_tokens=106
18:03:34,58 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:34,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4102232499280944. input_tokens=295, output_tokens=124
18:03:34,601 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:34,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5426964170765132. input_tokens=277, output_tokens=22
18:03:35,588 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:35,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9847761250566691. input_tokens=292, output_tokens=84
18:03:38,350 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:40,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7588686250383034. input_tokens=287, output_tokens=61
18:03:43,247 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:45,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8939578330609947. input_tokens=285, output_tokens=70
18:03:48,318 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:48,320 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.066881833015941. input_tokens=293, output_tokens=77
18:03:51,379 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:53,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0603799999225885. input_tokens=296, output_tokens=85
18:03:56,548 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:56,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1640790000092238. input_tokens=290, output_tokens=76
18:03:59,994 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:03:59,997 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4428400830365717. input_tokens=294, output_tokens=131
18:04:03,369 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:05,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3729344580788165. input_tokens=295, output_tokens=94
18:04:08,808 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:08,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4366734169889241. input_tokens=303, output_tokens=126
18:04:12,821 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:12,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0092456670245156. input_tokens=274, output_tokens=110
18:04:15,780 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:15,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9566499999491498. input_tokens=281, output_tokens=76
18:04:18,197 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:20,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41353845898993313. input_tokens=303, output_tokens=19
18:04:24,245 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:24,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0432998338947073. input_tokens=305, output_tokens=122
18:04:27,720 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:27,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4703481249744073. input_tokens=298, output_tokens=133
18:04:30,917 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:32,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1949403330218047. input_tokens=301, output_tokens=82
18:04:36,309 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:36,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.388550958945416. input_tokens=304, output_tokens=123
18:04:40,28 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:40,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.716424582991749. input_tokens=287, output_tokens=149
18:04:43,203 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:45,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1712324579712003. input_tokens=292, output_tokens=89
18:04:48,732 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:48,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5268356249434873. input_tokens=279, output_tokens=120
18:04:52,130 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:52,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3955169999971986. input_tokens=286, output_tokens=126
18:04:55,247 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:04:57,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1126557079842314. input_tokens=291, output_tokens=77
18:05:00,405 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:00,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1556607079692185. input_tokens=281, output_tokens=91
18:05:03,410 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:05,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0028545419918373. input_tokens=300, output_tokens=86
18:05:09,264 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:09,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.847492832923308. input_tokens=297, output_tokens=172
18:05:12,191 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:12,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.923931583063677. input_tokens=292, output_tokens=75
18:05:15,460 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:17,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2673310830723494. input_tokens=288, output_tokens=86
18:05:20,992 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:20,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5276420000009239. input_tokens=290, output_tokens=111
18:05:24,777 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:24,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.781128374976106. input_tokens=295, output_tokens=126
18:05:24,804 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
18:05:24,911 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
18:05:24,911 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
18:05:24,923 datashaper.workflow.workflow INFO executing verb cluster_graph
18:05:25,3 datashaper.workflow.workflow INFO executing verb select
18:05:25,11 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:05:25,85 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:05:25,86 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:05:25,114 datashaper.workflow.workflow INFO executing verb unpack_graph
18:05:25,139 datashaper.workflow.workflow INFO executing verb rename
18:05:25,142 datashaper.workflow.workflow INFO executing verb select
18:05:25,145 datashaper.workflow.workflow INFO executing verb dedupe
18:05:25,149 datashaper.workflow.workflow INFO executing verb rename
18:05:25,152 datashaper.workflow.workflow INFO executing verb filter
18:05:25,159 datashaper.workflow.workflow INFO executing verb text_split
18:05:25,163 datashaper.workflow.workflow INFO executing verb drop
18:05:25,167 datashaper.workflow.workflow INFO executing verb merge
18:05:25,181 datashaper.workflow.workflow INFO executing verb text_embed
18:05:25,183 graphrag.llm.openai.create_openai_client INFO Creating Azure OpenAI client api_base=https://aoai.openai.azure.com, deployment_name=text-embedding-ada-002
18:05:25,196 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-ada-002: TPM=0, RPM=60
18:05:25,196 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-ada-002: 1
18:05:25,204 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 237 inputs via 237 snippets using 15 batches. max_batch_size=16, max_tokens=8191
18:05:25,623 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:25,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6026771670440212. input_tokens=919, output_tokens=0
18:05:25,943 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:26,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19173179101198912. input_tokens=791, output_tokens=0
18:05:26,156 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:26,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19606387498788536. input_tokens=751, output_tokens=0
18:05:26,376 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:26,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19903987494762987. input_tokens=389, output_tokens=0
18:05:26,576 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:26,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19393637496978045. input_tokens=1017, output_tokens=0
18:05:26,786 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:26,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.1955488749081269. input_tokens=691, output_tokens=0
18:05:26,996 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:27,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19624245807062835. input_tokens=550, output_tokens=0
18:05:27,196 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:27,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.1868848749436438. input_tokens=599, output_tokens=0
18:05:27,414 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:27,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.1999377920292318. input_tokens=488, output_tokens=0
18:05:27,621 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:27,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19387950003147125. input_tokens=727, output_tokens=0
18:05:27,820 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:27,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19230062502902. input_tokens=513, output_tokens=0
18:05:28,35 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:28,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19542200001887977. input_tokens=608, output_tokens=0
18:05:28,240 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:28,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19603274995461106. input_tokens=386, output_tokens=0
18:05:28,453 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:28,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19105816597584635. input_tokens=340, output_tokens=0
18:05:28,646 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:28,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.19692579202819616. input_tokens=547, output_tokens=0
18:05:28,755 datashaper.workflow.workflow INFO executing verb drop
18:05:28,759 datashaper.workflow.workflow INFO executing verb filter
18:05:28,768 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:05:28,867 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:05:28,867 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:05:28,876 datashaper.workflow.workflow INFO executing verb layout_graph
18:05:28,965 datashaper.workflow.workflow INFO executing verb unpack_graph
18:05:28,995 datashaper.workflow.workflow INFO executing verb unpack_graph
18:05:29,22 datashaper.workflow.workflow INFO executing verb drop
18:05:29,26 datashaper.workflow.workflow INFO executing verb filter
18:05:29,38 datashaper.workflow.workflow INFO executing verb select
18:05:29,42 datashaper.workflow.workflow INFO executing verb rename
18:05:29,46 datashaper.workflow.workflow INFO executing verb convert
18:05:29,70 datashaper.workflow.workflow INFO executing verb join
18:05:29,78 datashaper.workflow.workflow INFO executing verb rename
18:05:29,84 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:05:29,166 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:05:29,166 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:05:29,177 datashaper.workflow.workflow INFO executing verb create_final_communities
18:05:29,230 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:05:29,322 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:05:29,323 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:05:29,325 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:05:29,337 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
18:05:29,363 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
18:05:29,367 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:05:29,452 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
18:05:29,452 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:05:29,462 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:05:29,463 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:05:29,478 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
18:05:29,487 datashaper.workflow.workflow INFO executing verb select
18:05:29,494 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:05:29,575 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:05:29,575 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:05:29,577 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:05:29,590 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
18:05:29,599 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
18:05:29,605 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
18:05:29,613 datashaper.workflow.workflow INFO executing verb prepare_community_reports
18:05:29,613 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 237
18:05:29,628 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 237
18:05:29,638 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 237
18:05:29,667 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 237
18:05:29,696 datashaper.workflow.workflow INFO executing verb create_community_reports
18:05:43,487 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:43,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.771427166997455. input_tokens=4794, output_tokens=1352
18:05:50,791 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:05:50,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.300680083106272. input_tokens=1647, output_tokens=710
18:05:50,887 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:05:50,888 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 21 seconds. Follow recommendation? True
18:06:27,714 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:06:27,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 14.593847166979685. input_tokens=4973, output_tokens=1395
18:06:27,793 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:06:27,794 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 3 seconds. Follow recommendation? True
18:06:39,531 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:06:39,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 7.658636791980825. input_tokens=1798, output_tokens=777
18:06:47,808 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:06:47,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.277471290901303. input_tokens=1751, output_tokens=643
18:06:55,922 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:06:55,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.108890375006013. input_tokens=1600, output_tokens=784
18:06:55,991 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:06:55,992 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 18 seconds. Follow recommendation? True
18:07:24,13 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:07:24,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 8.226658583967946. input_tokens=2227, output_tokens=755
18:07:24,84 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:07:24,85 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 8 seconds. Follow recommendation? True
18:07:41,793 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:07:41,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 8.160800165962428. input_tokens=1869, output_tokens=771
18:07:55,136 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:07:55,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 13.31566824996844. input_tokens=3467, output_tokens=1016
18:08:04,13 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:08:04,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.87268166593276. input_tokens=1870, output_tokens=777
18:08:04,84 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:08:04,85 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 13 seconds. Follow recommendation? True
18:08:28,788 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:08:28,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 9.991036333958618. input_tokens=2086, output_tokens=929
18:08:28,859 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:08:28,859 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 6 seconds. Follow recommendation? True
18:08:42,434 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:08:42,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 6.1698167090071365. input_tokens=1639, output_tokens=599
18:08:51,105 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:08:51,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.672344041988254. input_tokens=2189, output_tokens=850
18:08:51,183 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:08:51,184 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 5 seconds. Follow recommendation? True
18:09:06,990 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:09:06,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 9.035937167005613. input_tokens=1804, output_tokens=906
18:09:07,120 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:09:07,121 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 30 seconds. Follow recommendation? True
18:09:54,820 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:09:54,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 16.61920662492048. input_tokens=6052, output_tokens=1425
18:10:03,540 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:10:03,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.712426666985266. input_tokens=1607, output_tokens=817
18:10:10,786 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:10:10,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.245819499948993. input_tokens=1541, output_tokens=740
18:10:10,867 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:10:10,868 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 28 seconds. Follow recommendation? True
18:10:48,854 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:10:48,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 8.912814000039361. input_tokens=2016, output_tokens=807
18:10:57,584 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:10:57,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.725432459032163. input_tokens=2424, output_tokens=870
18:11:04,866 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:11:04,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.2791337500093505. input_tokens=1602, output_tokens=677
18:11:04,944 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:11:04,944 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 36 seconds. Follow recommendation? True
18:11:55,676 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:11:55,684 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 13.491126875043847. input_tokens=5539, output_tokens=1209
18:12:01,788 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:12:01,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.104562707943842. input_tokens=1729, output_tokens=630
18:12:09,839 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:12:09,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.04920141689945. input_tokens=1588, output_tokens=770
18:12:19,709 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:12:19,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.86717433296144. input_tokens=2596, output_tokens=788
18:12:19,782 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:12:19,783 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 23 seconds. Follow recommendation? True
18:12:52,883 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:12:52,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 9.02390629099682. input_tokens=1532, output_tokens=657
18:12:52,946 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:12:52,947 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 3 seconds. Follow recommendation? True
18:13:06,471 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:13:06,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 8.733427958912216. input_tokens=1543, output_tokens=682
18:13:11,216 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:13:11,219 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.742832665913738. input_tokens=1543, output_tokens=466
18:13:21,356 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:13:21,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.138287291978486. input_tokens=1949, output_tokens=946
18:13:30,366 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:13:30,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.008307125070132. input_tokens=2009, output_tokens=881
18:13:30,437 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:13:30,438 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 28 seconds. Follow recommendation? True
18:14:09,555 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:14:09,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 10.099340082961135. input_tokens=2549, output_tokens=886
18:14:17,848 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:14:17,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.293677542009391. input_tokens=1709, output_tokens=752
18:14:24,843 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:14:24,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.9908283749828115. input_tokens=1967, output_tokens=676
18:14:32,387 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:14:32,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.544318291009404. input_tokens=2336, output_tokens=781
18:14:32,458 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:14:32,459 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 28 seconds. Follow recommendation? True
18:15:09,355 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:15:09,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 7.8302500419085845. input_tokens=1557, output_tokens=743
18:15:09,429 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:15:09,430 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 1 seconds. Follow recommendation? True
18:15:22,348 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:15:22,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 10.168515041004866. input_tokens=1582, output_tokens=996
18:15:31,783 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:15:31,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.404643499990925. input_tokens=1857, output_tokens=779
18:15:39,358 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:15:39,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.573444874957204. input_tokens=2182, output_tokens=712
18:15:39,440 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:15:39,440 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 33 seconds. Follow recommendation? True
18:16:29,745 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:16:29,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 15.86395545792766. input_tokens=7755, output_tokens=1497
18:16:29,839 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:16:29,840 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 3 seconds. Follow recommendation? True
18:16:52,133 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:16:52,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 17.889954957994632. input_tokens=6865, output_tokens=1719
18:16:52,208 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:16:52,224 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 23 seconds. Follow recommendation? True
18:17:30,155 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:17:30,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 13.017336667049676. input_tokens=3784, output_tokens=1288
18:17:39,883 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:17:39,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.724518917035311. input_tokens=3055, output_tokens=983
18:17:47,44 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:17:47,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.157636082964018. input_tokens=1729, output_tokens=747
18:17:54,724 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:17:54,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.679123957990669. input_tokens=2226, output_tokens=768
18:17:54,822 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:17:54,823 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 23 seconds. Follow recommendation? True
18:18:30,162 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:18:30,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 11.01902741601225. input_tokens=2378, output_tokens=990
18:18:38,613 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:18:38,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.447866417001933. input_tokens=2390, output_tokens=829
18:18:38,687 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:18:38,688 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 2 seconds. Follow recommendation? True
18:18:54,21 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:18:54,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 12.125106791965663. input_tokens=3874, output_tokens=1204
18:19:00,598 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:19:00,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.574888083036058. input_tokens=2157, output_tokens=652
18:19:00,665 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 429 Too Many Requests"
18:19:00,666 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 30 seconds. Follow recommendation? True
18:19:41,638 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:19:41,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 9.172167207929306. input_tokens=2983, output_tokens=889
18:19:48,560 httpx INFO HTTP Request: POST https://aoai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-04-01-preview "HTTP/1.1 200 OK"
18:19:48,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.9181645000353456. input_tokens=1724, output_tokens=719
18:19:48,588 datashaper.workflow.workflow INFO executing verb window
18:19:48,596 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:19:48,772 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
18:19:48,773 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:19:48,809 datashaper.workflow.workflow INFO executing verb unroll
18:19:48,817 datashaper.workflow.workflow INFO executing verb select
18:19:48,824 datashaper.workflow.workflow INFO executing verb rename
18:19:48,830 datashaper.workflow.workflow INFO executing verb join
18:19:48,838 datashaper.workflow.workflow INFO executing verb aggregate_override
18:19:48,845 datashaper.workflow.workflow INFO executing verb join
18:19:48,854 datashaper.workflow.workflow INFO executing verb rename
18:19:48,861 datashaper.workflow.workflow INFO executing verb convert
18:19:48,869 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
18:19:48,963 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
18:19:48,964 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
18:19:48,997 datashaper.workflow.workflow INFO executing verb rename
18:19:49,2 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:19:49,30 graphrag.index.cli INFO All workflows completed successfully.
